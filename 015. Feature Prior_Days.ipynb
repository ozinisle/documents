{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "dataName = 'adani'\n",
    "dataFrequency = '1h'\n",
    "featureIndexStamp = \"013_\"\n",
    "variation_degree=20\n",
    "requiredMinimumCorrelation = .2\n",
    "autoConfigFileRelativePath = '\\\\src\\config\\\\autoConfig\\\\config.json'\n",
    "KEY_preProcessedDataFilePath='preProcessedDataFilePath'\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "into method doBasicOperation\n",
      "jupyterNodePath >>> C:\\Users\\kalagi01\\Desktop\\dev\\eraiVersion2\n",
      "configFilePath >>> C:\\Users\\kalagi01\\Desktop\\dev\\eraiVersion2\\src\\config\\autoConfig\\config.json\n",
      "added INPUT FEATURES >>> 4 count >>> open-high-low-close\n",
      "before return statement of method doBasicOperation \n"
     ]
    }
   ],
   "source": [
    "# Include python files defined in the library folder to help facilitate the stock price Prediction \n",
    "import os\n",
    "import sys  \n",
    "module_path = os.path.abspath('../../'+os.path.join('.'))\n",
    "\n",
    "sys.path.append(module_path)\n",
    "print('done')\n",
    "\n",
    "from dataPreparation import *\n",
    "from dataPreprocessing import *\n",
    "from utilities import *\n",
    "\n",
    "from dataPreparation.featurePreparation import doBasicOperation\n",
    "#from dataPreparation.featurePreparation import doFeatureAssessment\n",
    "\n",
    "_basicDf,_variation_degree,_preProcessedDataFilePath,_autoConfigData,_configFilePath = doBasicOperation(dataName,\n",
    "       dataFrequency,autoConfigFileRelativePath,KEY_preProcessedDataFilePath,variation_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary stuff\n",
    "import traceback\n",
    "\n",
    "traceback_template = '''Traceback (most recent call last):\n",
    "  File \"%(filename)s\", line %(lineno)s, in %(name)s\n",
    "%(type)s: %(message)s\\n''' # Skipping the \"actual line\" item\n",
    "\n",
    "#from dataPreparation.featurePreparation import createNewTrainingSetWithFeatureVariations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffixColumnsWithLabel(df,label):\n",
    "    import pandas as pd\n",
    "    ref=pd.concat([df],axis=1)\n",
    "    ref.columns+=label\n",
    "    return ref\n",
    "\n",
    "\n",
    "def doFeatureAssessment(newFeatureDf,basicDf,variation_degree,preProcessedDataFilePath,autoConfigData,\n",
    "    configFilePath,requiredMinimumCorrelation,featureIndexStamp,dataName,dataFrequency,useVersion2=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from utilities.fileFolderManipulations import getJupyterRootDirectory\n",
    "    from utilities.fileFolderManipulations import getParentFolder\n",
    "    from utilities.fileFolderManipulations import createFolder\n",
    "    from utilities.fileFolderManipulations import deleteFile\n",
    "    \n",
    "    from dataPreprocessing.basicRawDataProcess import setAutoConfigData\n",
    "    \n",
    "    try:\n",
    "\n",
    "        featureOfInterest = newFeatureDf.columns\n",
    "        if variation_degree==0:\n",
    "            newTrainingSetDf=pd.concat([basicDf,newFeatureDf],axis=1)\n",
    "        else:\n",
    "            if useVersion2 :\n",
    "                newTrainingSetDf = createNewTrainingSetWithFeatureVariations_v2(basicDf,newFeatureDf,featureOfInterest,variation_degree) \n",
    "            else :\n",
    "                newTrainingSetDf = createNewTrainingSetWithFeatureVariations(basicDf,newFeatureDf,featureOfInterest,variation_degree) \n",
    "\n",
    "        return newTrainingSetDf\n",
    "        correlation = newTrainingSetDf.corr()\n",
    "\n",
    "        reasonableCorelation = correlation.loc[ (np.abs(correlation['open'])>requiredMinimumCorrelation) & \n",
    "        (np.abs(correlation['high'])>requiredMinimumCorrelation) &\n",
    "        (np.abs(correlation['low'])>requiredMinimumCorrelation) & \n",
    "        (np.abs(correlation['close'])>requiredMinimumCorrelation)]\n",
    "\n",
    "        # create necessary file folder structure for storing and filtering features\n",
    "        preprocessedFolderPath = getParentFolder(preProcessedDataFilePath)\n",
    "        outputFolderPath = getParentFolder(preprocessedFolderPath)\n",
    "\n",
    "        featuresFolder = outputFolderPath+\"\\\\features\"\n",
    "        createFolder(featuresFolder)\n",
    "\n",
    "        rawFeaturesFolder = featuresFolder+\"\\\\rawFeatures\"\n",
    "        createFolder(rawFeaturesFolder)\n",
    "\n",
    "        filteredFeaturesFolder = featuresFolder+\"\\\\filteredFeatures\"\n",
    "        createFolder(filteredFeaturesFolder)\n",
    "\n",
    "        correlationsFolder = featuresFolder+\"\\\\correlations\"\n",
    "        createFolder(correlationsFolder)\n",
    "\n",
    "        reasonableCorrelationsFolder = correlationsFolder+\"\\\\reasonableCorrelations\"\n",
    "        createFolder(reasonableCorrelationsFolder)\n",
    "\n",
    "        trainableFeaturesListtFilePath = filteredFeaturesFolder+\"\\\\\"+featureIndexStamp+featureOfInterest+\"_trainableFeaturesList.csv\"\n",
    "        currentFeatureListFilePath = rawFeaturesFolder+\"\\\\\"+featureIndexStamp+featureOfInterest+\"_variations_list.csv\"\n",
    "        currentFeatureCorrelationListFilePath = correlationsFolder+\"\\\\\"+featureIndexStamp+featureOfInterest+\"_variations_correlation_list.csv\"\n",
    "        reasonableCorelationListFilePath = reasonableCorrelationsFolder+\"\\\\\"+featureIndexStamp+featureOfInterest+\"_variations_reasonable_correlation_list.csv\"\n",
    "\n",
    "        deleteFile(trainableFeaturesListtFilePath)\n",
    "        deleteFile(currentFeatureListFilePath)\n",
    "        deleteFile(currentFeatureCorrelationListFilePath)\n",
    "        deleteFile(reasonableCorelationListFilePath)\n",
    "\n",
    "        # store output information related to current \n",
    "        newTrainingSetDf.to_csv(currentFeatureListFilePath, sep=',', index=False)\n",
    "        correlation.to_csv(currentFeatureCorrelationListFilePath, sep=',', index=True)\n",
    "        reasonableCorelation.to_csv(reasonableCorelationListFilePath, sep=',', index=True)\n",
    "\n",
    "        if len(reasonableCorelation.index)>4:    \n",
    "            # store trainable features in global file - to be used by other training feature creation procedures    \n",
    "            newFilteredTrainableFeaturesDf = newTrainingSetDf[[filteredIndex for filteredIndex in reasonableCorelation.index] ]\n",
    "            trainableFeaturesDf=newFilteredTrainableFeaturesDf.drop(columns=[\"open\",\"close\",\"high\",\"low\"])    \n",
    "            # trainableFeaturesDf = getTrainableFeaturesListDf(trainableFeaturesListtFilePath)\n",
    "            # if trainableFeaturesDf is None:\n",
    "            #     trainableFeaturesDf= newFilteredTrainableFeaturesDf\n",
    "            # else:        \n",
    "            #     # newFilteredTrainableFeaturesDf=newFilteredTrainableFeaturesDf.drop(columns=[\"open\",\"close\",\"high\",\"low\"])    \n",
    "            #     # trainableFeaturesDf = pd.concat([trainableFeaturesDf,newFilteredTrainableFeaturesDf],axis=1)\n",
    "            #     for index in reasonableCorelation:\n",
    "            #         try:\n",
    "            #             trainableFeaturesDf[index] = newFilteredTrainableFeaturesDf[index]\n",
    "            #         except KeyError:\n",
    "            #             print ('key error >>>' + index)\n",
    "            \n",
    "            if not trainableFeaturesDf is None or trainableFeaturesDf.shape[1]>0:\n",
    "                trainableFeaturesDf.to_csv(trainableFeaturesListtFilePath, sep=',', index=False)\n",
    "\n",
    "            # assertions\n",
    "            print(\"newTrainingSetDf shape>>>\"+str(newTrainingSetDf.shape[0])+\",\"+str(newTrainingSetDf.shape[1]))\n",
    "            print(\"trainableFeaturesDf shape>>>\"+str(trainableFeaturesDf.shape[0])+\",\"+str(trainableFeaturesDf.shape[1]))\n",
    "            \n",
    "            autoConfigData[dataName][dataFrequency].update({'trainableFeaturesListtFile':trainableFeaturesListtFilePath})\n",
    "            setAutoConfigData(configFilePath,autoConfigData)\n",
    "        else:\n",
    "            trainableFeaturesDf = getTrainableFeaturesListDf(trainableFeaturesListtFilePath)\n",
    "\n",
    "        return correlation, reasonableCorelation ,newTrainingSetDf ,trainableFeaturesDf\n",
    "    except:\n",
    "        print(\"Error executing method >>> \")\n",
    "        # exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        # fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        # print(\"Unexpected error:\", sys.exc_info())\n",
    "        # print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        \n",
    "        # http://docs.python.org/2/library/sys.html#sys.exc_info\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info() # most recent (if any) by default\n",
    "        \n",
    "        '''\n",
    "        Reason this _can_ be bad: If an (unhandled) exception happens AFTER this,\n",
    "        or if we do not delete the labels on (not much) older versions of Py, the\n",
    "        reference we created can linger.\n",
    "\n",
    "        traceback.format_exc/print_exc do this very thing, BUT note this creates a\n",
    "        temp scope within the function.\n",
    "        '''\n",
    "\n",
    "        traceback_details = {\n",
    "                            'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                            'lineno'  : exc_traceback.tb_lineno,\n",
    "                            'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                            'type'    : exc_type.__name__,\n",
    "                            'message' : traceback.extract_tb(exc_traceback)\n",
    "                            }\n",
    "        \n",
    "        del(exc_type, exc_value, exc_traceback) # So we don't leave our local labels/objects dangling\n",
    "        # This still isn't \"completely safe\", though!\n",
    "        # \"Best (recommended) practice: replace all exc_type, exc_value, exc_traceback\n",
    "        # with sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\n",
    "        \n",
    "        print\n",
    "        print(traceback.format_exc())\n",
    "        print\n",
    "        print(traceback_template % traceback_details)\n",
    "        print\n",
    "\n",
    "        #traceback.print_exception()\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewTrainingSetWithFeatureVariations_v2(basicDf,newFeatureDf,featureOfInterest,variation_degree) :\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    try:\n",
    "        # Create and register a new `tqdm` instance with `pandas`\n",
    "        # (can use tqdm_gui, optional kwargs, etc.)\n",
    "        tqdm.pandas()\n",
    "\n",
    "\n",
    "        featureVariants=[[\n",
    "                            np.exp(suffixColumnsWithLabel(newFeatureDf,'_exp_'+str(iterator))*iterator), \n",
    "                            np.exp(suffixColumnsWithLabel(newFeatureDf,'_exp_inv_'+str(iterator))*iterator*-1),\n",
    "                            np.power(suffixColumnsWithLabel(newFeatureDf,'_pow_'+str(iterator)),iterator),\n",
    "                            np.power(suffixColumnsWithLabel(newFeatureDf,'_pow_inv_'+str(iterator)),iterator*-1)\n",
    "        ] for iterator in range(1,variation_degree+1)]\n",
    "        \n",
    "        \n",
    "        row=len(featureVariants[0][0])\n",
    "        col=len(featureVariants[0])\n",
    "        featureVariantCummList=[print(str(i)+\",\"+str(j)) for i in range(0,row-1) for j in range(0,col-1) if i>8300]\n",
    "        #featureVariantCummList=[featureVariants[i][j] for i in range(0,row-1) for j in range(0,col-1)]\n",
    "        \n",
    "        \n",
    "        return featureVariantCummList\n",
    "        newVariantsDf=pd.concat(featureVariantCummList,axis=1)\n",
    "        newTrainingSetDf= pd.concat([basicDf,newVariantsDf,newFeatureDf],axis=1)\n",
    "        #newTrainingSetDf= pd.concat([basicDf,newFeatureDf,newVariantsDf],axis=1)\n",
    "        \n",
    "        \n",
    "          \n",
    "        \n",
    "#         for iterator in range(1,variation_degree_count+1):\n",
    "#             row[featureName+'_exp_1'] = np.exp(featureVal)\n",
    "#             row[featureName+'_exp_inv_1'] = np.exp(-1*featureVal)\n",
    "        \n",
    "#             if iterator>1:\n",
    "#                 \n",
    "#                 if val>0:\n",
    "#                     row[featureName+'_log_times_'+str(iterator)] = np.log(val*iterator)\n",
    "#                 elif val<0:\n",
    "#                     row[featureName+'_log_times_'+str(iterator)] = -np.log(-1*val*iterator)\n",
    "            \n",
    "#         return row \n",
    "\n",
    "        \n",
    "    \n",
    "        return newTrainingSetDf\n",
    "    except:\n",
    "        print(\"Error executing method >>> \")\n",
    "        # exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        # fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        # print(\"Unexpected error:\", sys.exc_info())\n",
    "        # print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        \n",
    "        # http://docs.python.org/2/library/sys.html#sys.exc_info\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info() # most recent (if any) by default\n",
    "        \n",
    "        '''\n",
    "        Reason this _can_ be bad: If an (unhandled) exception happens AFTER this,\n",
    "        or if we do not delete the labels on (not much) older versions of Py, the\n",
    "        reference we created can linger.\n",
    "\n",
    "        traceback.format_exc/print_exc do this very thing, BUT note this creates a\n",
    "        temp scope within the function.\n",
    "        '''\n",
    "\n",
    "        traceback_details = {\n",
    "                            'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                            'lineno'  : exc_traceback.tb_lineno,\n",
    "                            'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                            'type'    : exc_type.__name__,\n",
    "                            'message' : traceback.extract_tb(exc_traceback)\n",
    "                            }\n",
    "        \n",
    "        del(exc_type, exc_value, exc_traceback) # So we don't leave our local labels/objects dangling\n",
    "        # This still isn't \"completely safe\", though!\n",
    "        # \"Best (recommended) practice: replace all exc_type, exc_value, exc_traceback\n",
    "        # with sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\n",
    "        \n",
    "        print\n",
    "        print(traceback.format_exc())\n",
    "        print\n",
    "        print(traceback_template % traceback_details)\n",
    "        print\n",
    "\n",
    "        #traceback.print_exception()\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8301,0\n",
      "8301,1\n",
      "8301,2\n",
      "8302,0\n",
      "8302,1\n",
      "8302,2\n",
      "8303,0\n",
      "8303,1\n",
      "8303,2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "featureOfInterest = 'prior_days'\n",
    "_basicDf=_basicDf[1:10]\n",
    "#newFeatureDf = ((_basicDf['open']+_basicDf['high'])/2).rename(featureOfInterest)\n",
    "inputRawProcessedDataDF = pd.read_csv(_preProcessedDataFilePath) \n",
    "\n",
    "# newFeatureDf = np.divide(np.sqrt(np.sum(np.square(inputRawProcessedDataDF[['open','close','high','low']]),axis=1)),4)\n",
    "# newFeatureDf = newFeatureDf.rename(featureOfInterest)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "featureName=['prior_holidays','scarcity_by_prior_holidays','magnitudeTimesScarcity_by_prior_holidays']\n",
    "\n",
    "priorHolidaysDataDf = inputRawProcessedDataDF[['open','close','high','low','prior_holidays']]\n",
    "m = priorHolidaysDataDf.shape[0]\n",
    "\n",
    "prior_holidays_unique_values = priorHolidaysDataDf[featureName[0]].unique()\n",
    "\n",
    "dataByScarcityOfPriorHolidays=priorHolidaysDataDf[featureName[0]].rename(featureName[1])\n",
    "\n",
    "priorHolidaysDataDf=pd.concat([priorHolidaysDataDf,dataByScarcityOfPriorHolidays],axis=1)\n",
    "\n",
    "for unq_val in prior_holidays_unique_values:\n",
    "    matches=priorHolidaysDataDf.loc[priorHolidaysDataDf[featureName[0]]==unq_val]    \n",
    "    priorHolidaysDataDf[featureName[0]][priorHolidaysDataDf[featureName[1]]==unq_val]=100*(m-matches.shape[0])/m\n",
    "  \n",
    "magnitudeDf= np.divide(np.sqrt(np.sum(np.square(priorHolidaysDataDf[['open','close','high','low']]),axis=1)),4)\n",
    "\n",
    "magnitudeTimesScarcityDf = np.multiply(magnitudeDf,dataByScarcityOfPriorHolidays)\n",
    "\n",
    "priorHolidaysDataDf=pd.concat([priorHolidaysDataDf,magnitudeTimesScarcityDf.rename(featureName[2])],axis=1)            \n",
    "\n",
    "newFeatureDf=priorHolidaysDataDf.drop({'open','close','high','low'},axis=1)\n",
    "#priorHolidaysDataDf.corr()\n",
    "\n",
    "_basicDf=inputRawProcessedDataDF[['open','close','high','low']]\n",
    "\n",
    "# _correlation, _reasonableCorelation ,_newTrainingSetDf ,_trainableFeaturesDf = doFeatureAssessment(newFeatureDf,\n",
    "#             _basicDf,_variation_degree,_preProcessedDataFilePath,_autoConfigData,\n",
    "#             _configFilePath,requiredMinimumCorrelation,featureIndexStamp,\n",
    "#             dataName,dataFrequency,useVersion2=True)\n",
    "_trainableFeaturesDf = doFeatureAssessment(newFeatureDf,\n",
    "            _basicDf,_variation_degree,_preProcessedDataFilePath,_autoConfigData,\n",
    "            _configFilePath,requiredMinimumCorrelation,featureIndexStamp,\n",
    "            dataName,dataFrequency,useVersion2=True)\n",
    "_trainableFeaturesDf\n",
    "\n",
    "#newFeatureDf.columns\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8350"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_trainableFeaturesDf[0])\n",
    "len(_trainableFeaturesDf[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function len(obj, /)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0 -1 -1  1\n",
       "1  0  2  0\n",
       "2  3  3 -3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFeatureDf=pd.DataFrame({\"a\":[-1,0,3],'b':[-1,2,3],'c':[1,0,-3]})\n",
    "newFeatureDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.exp(df,[ iterator for iterator in range(variation_degree+1)])\n",
    "#pd.concat([np.exp((df.rename(columns={\"a\":\"a_sq\"}))*2),np.exp(df*3)],axis=1)\n",
    "\n",
    "# mdf=df\n",
    "# for logItr in range(21)\n",
    "# mdf[mdf<0]=-np.log(-1*mdf[mdf<0])\n",
    "# mdf[mdf>0]=np.log(mdf[mdf>0])\n",
    "# #np.log(2*df)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-07610f45c7d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;34m\"_test\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns+=\"_test\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "x.append(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
